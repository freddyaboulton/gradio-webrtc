{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"FastRTC  The Real-Time Communication Library for Python.   <p>Turn any python function into a real-time audio and video stream over WebRTC or WebSockets.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install fastrtc\n</code></pre> <p>to use built-in pause detection (see ReplyOnPause), speech-to-text (see Speech To Text), and text to speech (see Text To Speech), install the <code>vad</code>, <code>stt</code>, and <code>tts</code> extras:</p> <pre><code>pip install fastrtc[vad, stt, tts]\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Import the Stream class and pass in a handler. The <code>Stream</code> has three main methods:</p> <ul> <li><code>.ui.launch()</code>: Launch a built-in UI for easily testing and sharing your stream. Built with Gradio.</li> <li><code>.fastphone()</code>: Get a free temporary phone number to call into your stream. Hugging Face token required.</li> <li><code>.mount(app)</code>: Mount the stream on a FastAPI app. Perfect for integrating with your already existing production system.</li> </ul> Echo AudioLLM Voice ChatWebcam StreamObject Detection <pre><code>from fastrtc import Stream, ReplyOnPause\nimport numpy as np\n\ndef echo(audio: tuple[int, np.ndarray]):\n    # The function will be passed the audio until the user pauses\n    # Implement any iterator that yields audio\n    # See \"LLM Voice Chat\" for a more complete example\n    yield audio\n\nstream = Stream(\n    handler=ReplyOnPause(echo),\n    modality=\"audio\", \n    mode=\"send-receive\",\n)\n</code></pre> <pre><code>import os\n\nfrom fastrtc import (ReplyOnPause, Stream, get_stt_model, get_tts_model)\nfrom openai import OpenAI\n\nsambanova_client = OpenAI(\n    api_key=os.getenv(\"SAMBANOVA_API_KEY\"), base_url=\"https://api.sambanova.ai/v1\"\n)\nstt_model = get_stt_model()\ntts_model = get_tts_model()\n\ndef echo(audio):\n    prompt = stt_model.stt(audio)\n    response = sambanova_client.chat.completions.create(\n        model=\"Meta-Llama-3.2-3B-Instruct\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=200,\n    )\n    prompt = response.choices[0].message.content\n    for audio_chunk in tts_model.stream_tts_sync(prompt):\n        yield audio_chunk\n\nstream = Stream(ReplyOnPause(echo), modality=\"audio\", mode=\"send-receive\")\n</code></pre> <pre><code>from fastrtc import Stream\nimport numpy as np\n\n\ndef flip_vertically(image):\n    return np.flip(image, axis=0)\n\n\nstream = Stream(\n    handler=flip_vertically,\n    modality=\"video\",\n    mode=\"send-receive\",\n)\n</code></pre> <pre><code>from fastrtc import Stream\nimport gradio as gr\nimport cv2\nfrom huggingface_hub import hf_hub_download\nfrom .inference import YOLOv10\n\nmodel_file = hf_hub_download(\n    repo_id=\"onnx-community/yolov10n\", filename=\"onnx/model.onnx\"\n)\n\n# git clone https://huggingface.co/spaces/fastrtc/object-detection\n# for YOLOv10 implementation\nmodel = YOLOv10(model_file)\n\ndef detection(image, conf_threshold=0.3):\n    image = cv2.resize(image, (model.input_width, model.input_height))\n    new_image = model.detect_objects(image, conf_threshold)\n    return cv2.resize(new_image, (500, 500))\n\nstream = Stream(\n    handler=detection,\n    modality=\"video\", \n    mode=\"send-receive\",\n    additional_inputs=[\n        gr.Slider(minimum=0, maximum=1, step=0.01, value=0.3)\n    ]\n)\n</code></pre> <p>Run:</p> UITelephoneFastAPI <pre><code>stream.ui.launch()\n</code></pre> <pre><code>stream.fastphone()\n</code></pre> <pre><code>app = FastAPI()\nstream.mount(app)\n\n# Optional: Add routes\n@app.get(\"/\")\nasync def _():\n    return HTMLResponse(content=open(\"index.html\").read())\n\n# uvicorn app:app --host 0.0.0.0 --port 8000\n</code></pre> <p>Learn more about the Stream in the user guide.</p>"},{"location":"#key-features","title":"Key Features","text":"<p> Automatic Voice Detection and Turn Taking built-in, only worry about the logic for responding to the user.</p> <p> Automatic UI - Use the <code>.ui.launch()</code> method to launch the webRTC-enabled built-in Gradio UI.</p> <p> Automatic WebRTC Support - Use the <code>.mount(app)</code> method to mount the stream on a FastAPI app and get a webRTC endpoint for your own frontend! </p> <p> Websocket Support - Use the <code>.mount(app)</code> method to mount the stream on a FastAPI app and get a websocket endpoint for your own frontend! </p> <p> Automatic Telephone Support - Use the <code>fastphone()</code> method of the stream to launch the application and get a free temporary phone number!</p> <p> Completely customizable backend - A <code>Stream</code> can easily be mounted on a FastAPI app so you can easily extend it to fit your production application. See the Talk To Claude demo for an example on how to serve a custom JS frontend.</p>"},{"location":"#examples","title":"Examples","text":"<p>See the cookbook.</p> <p>Follow and join or organization on Hugging Face!</p>"},{"location":"advanced-configuration/","title":"Advanced Configuration","text":"<p>Any of the parameters for the <code>Stream</code> class can be passed to the <code>WebRTC</code> component directly.</p>"},{"location":"advanced-configuration/#track-constraints","title":"Track Constraints","text":"<p>You can specify the <code>track_constraints</code> parameter to control how the data is streamed to the server. The full documentation on track constraints is here.</p> <p>For example, you can control the size of the frames captured from the webcam like so:</p> <pre><code>track_constraints = {\n    \"width\": {\"exact\": 500},\n    \"height\": {\"exact\": 500},\n    \"frameRate\": {\"ideal\": 30},\n}\nwebrtc = Stream(\n    handler=...,\n    track_constraints=track_constraints,\n    modality=\"video\",\n    mode=\"send-receive\")\n</code></pre> <p>Warning</p> <p>WebRTC may not enforce your constraints. For example, it may rescale your video (while keeping the same resolution) in order to maintain the desired frame rate (or reach a better one). If you really want to enforce height, width and resolution constraints, use the <code>rtp_params</code> parameter as set <code>\"degradationPreference\": \"maintain-resolution\"</code>. </p> <pre><code>image = Stream(\n    modality=\"video\",\n    mode=\"send\",\n    track_constraints=track_constraints,\n    rtp_params={\"degradationPreference\": \"maintain-resolution\"}\n)\n</code></pre>"},{"location":"advanced-configuration/#the-rtc-configuration","title":"The RTC Configuration","text":"<p>You can configure how the connection is created on the client by passing an <code>rtc_configuration</code> parameter to the <code>WebRTC</code> component constructor. See the list of available arguments here.</p> <p>Warning</p> <p>When deploying on a remote server, the <code>rtc_configuration</code> parameter must be passed in. See Deployment.</p>"},{"location":"advanced-configuration/#reply-on-pause-voice-activity-detection","title":"Reply on Pause Voice-Activity-Detection","text":"<p>The <code>ReplyOnPause</code> class runs a Voice Activity Detection (VAD) algorithm to determine when a user has stopped speaking.</p> <ol> <li>First, the algorithm determines when the user has started speaking.</li> <li>Then it groups the audio into chunks.</li> <li>On each chunk, we determine the length of human speech in the chunk.</li> <li>If the length of human speech is below a threshold, a pause is detected.</li> </ol> <p>The following parameters control this argument:</p> <pre><code>from fastrtc import AlgoOptions, ReplyOnPause, Stream\n\noptions = AlgoOptions(audio_chunk_duration=0.6, # (1)\n                      started_talking_threshold=0.2, # (2)\n                      speech_threshold=0.1, # (3)\n                      )\n\nStream(\n    handler=ReplyOnPause(..., algo_options=algo_options),\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n</code></pre> <ol> <li>This is the length (in seconds) of audio chunks.</li> <li>If the chunk has more than 0.2 seconds of speech, the user started talking.</li> <li>If, after the user started speaking, there is a chunk with less than 0.1 seconds of speech, the user stopped speaking.</li> </ol>"},{"location":"advanced-configuration/#stream-handler-input-audio","title":"Stream Handler Input Audio","text":"<p>You can configure the sampling rate of the audio passed to the <code>ReplyOnPause</code> or <code>StreamHandler</code> instance with the <code>input_sampling_rate</code> parameter. The current default is <code>48000</code></p> <pre><code>from fastrtc import ReplyOnPause, Stream\n\nstream = Stream(\n    handler=ReplyOnPause(..., input_sampling_rate=24000),\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n</code></pre>"},{"location":"advanced-configuration/#stream-handler-output-audio","title":"Stream Handler Output Audio","text":"<p>You can configure the output audio chunk size of <code>ReplyOnPause</code> (and any <code>StreamHandler</code>)  with the <code>output_sample_rate</code> and <code>output_frame_size</code> parameters.</p> <p>The following code (which uses the default values of these parameters), states that each output chunk will be a frame of 960 samples at a frame rate of <code>24,000</code> hz. So it will correspond to <code>0.04</code> seconds.</p> <pre><code>from fastrtc import ReplyOnPause, Stream\n\nstream = Stream(\n    handler=ReplyOnPause(..., output_sample_rate=24000, output_frame_size=960),\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n</code></pre> <p>Tip</p> <p>In general it is best to leave these settings untouched. In some cases, lowering the output_frame_size can yield smoother audio playback.</p>"},{"location":"advanced-configuration/#audio-icon","title":"Audio Icon","text":"<p>You can display an icon of your choice instead of the default wave animation for audio streaming. Pass any local path or url to an image (svg, png, jpeg) to the components <code>icon</code> parameter. This will display the icon as a circular button. When audio is sent or recevied (depending on the <code>mode</code> parameter) a pulse animation will emanate from the button.</p> <p>You can control the button color and pulse color with <code>icon_button_color</code> and <code>pulse_color</code> parameters. They can take any valid css color.</p> <p>Warning</p> <p>The <code>icon</code> parameter is only supported in the <code>WebRTC</code> component.</p> CodeCode Custom colors <p><pre><code>audio = WebRTC(\n    label=\"Stream\",\n    rtc_configuration=rtc_configuration,\n    mode=\"receive\",\n    modality=\"audio\",\n    icon=\"phone-solid.svg\",\n)\n</code></pre> </p> <p><pre><code>audio = WebRTC(\n    label=\"Stream\",\n    rtc_configuration=rtc_configuration,\n    mode=\"receive\",\n    modality=\"audio\",\n    icon=\"phone-solid.svg\",\n    icon_button_color=\"black\",\n    pulse_color=\"black\",\n)\n</code></pre> </p>"},{"location":"advanced-configuration/#changing-the-button-text","title":"Changing the Button Text","text":"<p>You can supply a <code>button_labels</code> dictionary to change the text displayed in the <code>Start</code>, <code>Stop</code> and <code>Waiting</code> buttons that are displayed in the UI. The keys must be <code>\"start\"</code>, <code>\"stop\"</code>, and <code>\"waiting\"</code>.</p> <p>Warning</p> <p>The <code>button_labels</code> parameter is only supported in the <code>WebRTC</code> component.</p> <pre><code>webrtc = WebRTC(\n    label=\"Video Chat\",\n    modality=\"audio-video\",\n    mode=\"send-receive\",\n    button_labels={\"start\": \"Start Talking to Gemini\"}\n)\n</code></pre> <p></p>"},{"location":"cookbook/","title":"Cookbook","text":"<p>A collection of applications built with FastRTC. Click on the tags below to find the app you're looking for!</p> <code>audio</code> <code>video</code> <code>llm</code> <code>computer-vision</code> <code>real-time-api</code> <code>voice chat</code> <code>code generation</code> <code>stopword</code> <code>transcription</code> <code>SambaNova</code> <code>Groq</code> <code>ElevenLabs</code> <code>Kyutai</code> <code>Agentic</code> <ul> <li> <p> Gemini Audio Video Chat</p> <p>Stream BOTH your webcam video and audio feeds to Google Gemini. You can also upload images to augment your conversation!</p> <p></p> <p> Demo</p> <p> Gradio UI</p> <p> Code</p> </li> <li> <p> Google Gemini Real Time Voice API</p> <p>Talk to Gemini in real time using Google's voice API.</p> <p></p> <p> Demo</p> <p> Gradio UI</p> <p> Code</p> </li> <li> <p> OpenAI Real Time Voice API</p> <p>Talk to ChatGPT in real time using OpenAI's voice API.</p> <p></p> <p> Demo</p> <p> Gradio UI</p> <p> Code</p> </li> <li> <p> Hello Computer</p> <p>Say computer before asking your question! </p> <p> Demo</p> <p> Gradio UI</p> <p> Code</p> </li> <li> <p> Llama Code Editor</p> <p>Create and edit HTML pages with just your voice! Powered by Groq!</p> <p></p> <p> Demo</p> <p> Code</p> </li> <li> <p> SmolAgents with Voice</p> <p>Build a voice-based smolagent to find a coworking space!</p> <p></p> <p> Demo</p> <p> Gradio UI</p> <p> Code</p> </li> <li> <p> Talk to Claude</p> <p>Use the Anthropic and Play.Ht APIs to have an audio conversation with Claude.</p> <p></p> <p> Demo</p> <p> Gradio UI</p> <p> Code</p> </li> <li> <p> LLM Voice Chat</p> <p>Talk to an LLM with ElevenLabs!</p> <p></p> <p> Demo</p> <p> Gradio UI</p> <p> Code</p> </li> <li> <p> Whisper Transcription</p> <p>Have whisper transcribe your speech in real time!</p> <p></p> <p> Demo</p> <p> Gradio UI</p> <p> Code</p> </li> <li> <p> Talk to Sambanova</p> <p>Talk to Llama 3.2 with the SambaNova API. </p> <p> Demo</p> <p> Gradio UI</p> <p> Code</p> </li> <li> <p> Hello Llama: Stop Word Detection</p> <p>A code editor built with Llama 3.3 70b that is triggered by the phrase \"Hello Llama\". Build a Siri-like coding assistant in 100 lines of code!</p> <p></p> <p> Demo</p> <p> Code</p> </li> <li> <p> Audio Input/Output with mini-omni2</p> <p>Build a GPT-4o like experience with mini-omni2, an audio-native LLM.</p> <p></p> <p> Demo</p> <p> Code</p> </li> <li> <p> Kyutai Moshi</p> <p>Kyutai's moshi is a novel speech-to-speech model for modeling human conversations.</p> <p></p> <p> Demo</p> <p> Code</p> </li> <li> <p> Talk to Ultravox</p> <p>Talk to Fixie.AI's audio-native Ultravox LLM with the transformers library.</p> <p></p> <p> Demo</p> <p> Code</p> </li> <li> <p> Talk to Llama 3.2 3b</p> <p>Use the Lepton API to make Llama 3.2 talk back to you!</p> <p></p> <p> Demo</p> <p> Code</p> </li> <li> <p> Talk to Qwen2-Audio</p> <p>Qwen2-Audio is a SOTA audio-to-text LLM developed by Alibaba.</p> <p></p> <p> Demo</p> <p> Code</p> </li> <li> <p> Yolov10 Object Detection</p> <p>Run the Yolov10 model on a user webcam stream in real time!</p> <p></p> <p> Demo</p> <p> Code</p> </li> <li> <p> Video Object Detection with RT-DETR</p> <p>Upload a video and stream out frames with detected objects (powered by RT-DETR) model.</p> <p> Demo</p> <p> Code</p> </li> <li> <p> Text-to-Speech with Parler</p> <p>Stream out audio generated by Parler TTS!</p> <p> Demo</p> <p> Code</p> </li> </ul>"},{"location":"deployment/","title":"Deployment","text":"<p>When deploying in cloud environments with firewalls (like Hugging Face Spaces, RunPod), your WebRTC connections may be blocked from making direct connections. In these cases, you need a TURN server to relay the audio/video traffic between users. This guide covers different options for setting up FastRTC to connect to a TURN server.</p> <p>Tip</p> <p>The <code>rtc_configuration</code> parameter of the <code>Stream</code> class also be passed to the <code>WebRTC</code> component directly if you're building a standalone gradio app.</p>"},{"location":"deployment/#community-server","title":"Community Server","text":"<p>Hugging Face graciously provides a TURN server for the community. In order to use it, you need to first create a Hugging Face account by going to huggingface.co.</p> <p>Then navigate to this space and follow the instructions on the page. You just have to click the \"Log in\" button and then the \"Sign Up\" button.</p> <p></p> <p>Then you can use the <code>get_hf_turn_credentials</code> helper to get your credentials:</p> <pre><code>from fastrtc import get_hf_turn_credentials, Stream\n\n# Pass a valid access token for your Hugging Face account\n# or set the HF_TOKEN environment variable \ncredentials = get_hf_turn_credentials(token=None)\n\nStream(\n    handler=...,\n    rtc_configuration=credentials,\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n</code></pre> <p>Warning</p> <p>This is a shared resource so we make no latency/availability guarantees. For more robust options, see the Twilio, Cloudflare and self-hosting options below.</p>"},{"location":"deployment/#twilio-api","title":"Twilio API","text":"<p>An easy way to do this is to use a service like Twilio.</p> <p>Create a free account and the install the <code>twilio</code> package with pip (<code>pip install twilio</code>). You can then connect from the WebRTC component like so:</p> <pre><code>from fastrtc import Stream\nfrom twilio.rest import Client\nimport os\n\naccount_sid = os.environ.get(\"TWILIO_ACCOUNT_SID\")\nauth_token = os.environ.get(\"TWILIO_AUTH_TOKEN\")\n\nclient = Client(account_sid, auth_token)\n\ntoken = client.tokens.create()\n\nrtc_configuration = {\n    \"iceServers\": token.ice_servers,\n    \"iceTransportPolicy\": \"relay\",\n}\n\nStream(\n    handler=...,\n    rtc_configuration=rtc_configuration,\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n</code></pre> <p>Automatic login</p> <p>You can log in automatically with the <code>get_twilio_turn_credentials</code> helper</p> <pre><code>from gradio_webrtc import get_twilio_turn_credentials\n\n# Will automatically read the TWILIO_ACCOUNT_SID and TWILIO_AUTH_TOKEN\n# env variables but you can also pass in the tokens as parameters\nrtc_configuration = get_twilio_turn_credentials()\n</code></pre>"},{"location":"deployment/#cloudflare-calls-api","title":"Cloudflare Calls API","text":"<p>Cloudflare also offers a managed TURN server with Cloudflare Calls.</p> <p>Create a free account and head to the Calls section in your dashboard.</p> <p>Choose <code>Create -&gt; TURN App</code>, give it a name (like <code>fastrtc-demo</code>), and then hit the Create button.</p> <p>Take note of the Turn Token ID (often exported as <code>TURN_KEY_ID</code>) and API Token (exported as <code>TURN_KEY_API_TOKEN</code>).</p> <p>You can then connect from the WebRTC component like so:</p> <pre><code>from fastrtc import Stream\nimport requests\nimport os\n\nturn_key_id = os.environ.get(\"TURN_KEY_ID\")\nturn_key_api_token = os.environ.get(\"TURN_KEY_API_TOKEN\")\nttl = 86400 # Can modify TTL, here it's set to 24 hours\n\nresponse = requests.post(\n    f\"https://rtc.live.cloudflare.com/v1/turn/keys/{turn_key_id}/credentials/generate\",\n    headers={\n        \"Authorization\": f\"Bearer {turn_key_api_token}\",\n        \"Content-Type\": \"application/json\",\n    },\n    json={\"ttl\": ttl},  \n)\nif response.ok:\n    ice_servers = [response.json()[\"iceServers\"]]\nelse:\n    raise Exception(\n        f\"Failed to get TURN credentials: {response.status_code} {response.text}\"\n    )\n\nrtc_configuration = {\"iceServers\": ice_servers}\n\nstream = Stream(\n    handler=...,\n    rtc_configuration=rtc_configuration,\n    modality=\"audio\",\n    mode=\"send-receive\",\n)\n</code></pre>"},{"location":"deployment/#self-hosting","title":"Self Hosting","text":"<p>We have developed a script that can automatically deploy a TURN server to Amazon Web Services (AWS). You can follow the instructions here or this guide.</p>"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":"<p>Clone the following repository and install the <code>aws</code> cli if you have not done so already (<code>pip install awscli</code>).</p> <p>Log into your AWS account and create an IAM user with the following permissions:</p> <ul> <li>AWSCloudFormationFullAccess</li> <li>AmazonEC2FullAccess</li> </ul> <p>Create a key pair for this user and write down the \"access key\" and \"secret access key\". Then log into the aws cli with these credentials (<code>aws configure</code>).</p> <p>Finally, create an ec2 keypair (replace <code>your-key-name</code> with the name you want to give it).</p> <pre><code>aws ec2 create-key-pair --key-name your-key-name --query 'KeyMaterial' --output text &gt; your-key-name.pem\n</code></pre>"},{"location":"deployment/#running-the-script","title":"Running the script","text":"<p>Open the <code>parameters.json</code> file and fill in the correct values for all the parameters:</p> <ul> <li><code>KeyName</code>: The key file we just created, e.g. <code>your-key-name</code> (omit <code>.pem</code>).</li> <li><code>TurnUserName</code>: The username needed to connect to the server.</li> <li><code>TurnPassword</code>: The password needed to connect to the server.</li> <li><code>InstanceType</code>: One of the following values <code>t3.micro</code>, <code>t3.small</code>, <code>t3.medium</code>, <code>c4.large</code>, <code>c5.large</code>.</li> </ul> <p>Then run the deployment script:</p> <pre><code>aws cloudformation create-stack \\\n  --stack-name turn-server \\\n  --template-body file://deployment.yml \\\n  --parameters file://parameters.json \\\n  --capabilities CAPABILITY_IAM\n</code></pre> <p>You can then wait for the stack to come up with:</p> <pre><code>aws cloudformation wait stack-create-complete \\\n  --stack-name turn-server\n</code></pre> <p>Next, grab your EC2 server's public ip with:</p> <pre><code>aws cloudformation describe-stacks \\\n  --stack-name turn-server \\\n  --query 'Stacks[0].Outputs' &gt; server-info.json\n</code></pre> <p>The <code>server-info.json</code> file will have the server's public IP and public DNS:</p> <pre><code>[\n    {\n        \"OutputKey\": \"PublicIP\",\n        \"OutputValue\": \"35.173.254.80\",\n        \"Description\": \"Public IP address of the TURN server\"\n    },\n    {\n        \"OutputKey\": \"PublicDNS\",\n        \"OutputValue\": \"ec2-35-173-254-80.compute-1.amazonaws.com\",\n        \"Description\": \"Public DNS name of the TURN server\"\n    }\n]\n</code></pre> <p>Finally, you can connect to your EC2 server from the gradio WebRTC component via the <code>rtc_configuration</code> argument:</p> <pre><code>from fastrtc import Stream\nrtc_configuration = {\n    \"iceServers\": [\n        {\n            \"urls\": \"turn:35.173.254.80:80\",\n            \"username\": \"&lt;my-username&gt;\",\n            \"credential\": \"&lt;my-password&gt;\"\n        },\n    ]\n}\nStream(\n    handler=...,\n    rtc_configuration=rtc_configuration,\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#demo-does-not-work-when-deploying-to-the-cloud","title":"Demo does not work when deploying to the cloud","text":"<p>Make sure you are using a TURN server. See deployment.</p>"},{"location":"faq/#recorded-input-audio-sounds-muffled-during-output-audio-playback","title":"Recorded input audio sounds muffled during output audio playback","text":"<p>By default, the microphone is configured to do echo cancellation. This is what's causing the recorded audio to sound muffled when the streamed audio starts playing. You can disable this via the <code>track_constraints</code> (see Advanced Configuration) with the following code:</p> <pre><code>stream = Stream(\n    track_constraints={\n            \"echoCancellation\": False,\n            \"noiseSuppression\": {\"exact\": True},\n            \"autoGainControl\": {\"exact\": True},\n            \"sampleRate\": {\"ideal\": 24000},\n            \"sampleSize\": {\"ideal\": 16},\n            \"channelCount\": {\"exact\": 1},\n        },\n    rtc_configuration=None,\n    mode=\"send-receive\",\n    modality=\"audio\",\n)\n</code></pre>"},{"location":"faq/#how-to-raise-errors-in-the-ui","title":"How to raise errors in the UI","text":"<p>You can raise <code>WebRTCError</code> in order for an error message to show up in the user's screen. This is similar to how <code>gr.Error</code> works.</p> <p>Warning</p> <p>The <code>WebRTCError</code> class is only supported in the <code>WebRTC</code> component.</p> <p>Here is a simple example:</p> <pre><code>def generation(num_steps):\n    for _ in range(num_steps):\n        segment = AudioSegment.from_file(\n            \"/Users/freddy/sources/gradio/demo/audio_debugger/cantina.wav\"\n        )\n        yield (\n            segment.frame_rate,\n            np.array(segment.get_array_of_samples()).reshape(1, -1),\n        )\n        time.sleep(3.5)\n    raise WebRTCError(\"This is a test error\")\n\nwith gr.Blocks() as demo:\n    audio = WebRTC(\n    label=\"Stream\",\n    mode=\"receive\",\n    modality=\"audio\",\n    )\n    num_steps = gr.Slider(\n        label=\"Number of Steps\",\n        minimum=1,\n        maximum=10,\n        step=1,\n        value=5,\n    )\n    button = gr.Button(\"Generate\")\n\n    audio.stream(\n        fn=generation, inputs=[num_steps], outputs=[audio], trigger=button.click\n    )\n\ndemo.launch()\n</code></pre>"},{"location":"utils/","title":"Utils","text":""},{"location":"utils/#audio_to_bytes","title":"<code>audio_to_bytes</code>","text":"<p>Convert an audio tuple containing sample rate and numpy array data into bytes. Useful for sending data to external APIs from <code>ReplyOnPause</code> handler.</p> <p>Parameters <pre><code>audio : tuple[int, np.ndarray]\n    A tuple containing:\n        - sample_rate (int): The audio sample rate in Hz\n        - data (np.ndarray): The audio data as a numpy array\n</code></pre></p> <p>Returns <pre><code>bytes\n    The audio data encoded as bytes, suitable for transmission or storage\n</code></pre></p> <p>Example <pre><code>&gt;&gt;&gt; sample_rate = 44100\n&gt;&gt;&gt; audio_data = np.array([0.1, -0.2, 0.3])  # Example audio samples\n&gt;&gt;&gt; audio_tuple = (sample_rate, audio_data)\n&gt;&gt;&gt; audio_bytes = audio_to_bytes(audio_tuple)\n</code></pre></p>"},{"location":"utils/#audio_to_file","title":"<code>audio_to_file</code>","text":"<p>Save an audio tuple containing sample rate and numpy array data to a file.</p> <p>Parameters <pre><code>audio : tuple[int, np.ndarray]\n    A tuple containing:\n        - sample_rate (int): The audio sample rate in Hz\n        - data (np.ndarray): The audio data as a numpy array\n</code></pre> Returns <pre><code>str\n    The path to the saved audio file\n</code></pre> Example <pre><code>```python\n&gt;&gt;&gt; sample_rate = 44100\n&gt;&gt;&gt; audio_data = np.array([0.1, -0.2, 0.3])  # Example audio samples\n&gt;&gt;&gt; audio_tuple = (sample_rate, audio_data)\n&gt;&gt;&gt; file_path = audio_to_file(audio_tuple)\n&gt;&gt;&gt; print(f\"Audio saved to: {file_path}\")\n</code></pre></p>"},{"location":"utils/#aggregate_bytes_to_16bit","title":"<code>aggregate_bytes_to_16bit</code>","text":"<p>Aggregate bytes to 16-bit audio samples.</p> <p>This function takes an iterator of chunks and aggregates them into 16-bit audio samples. It handles incomplete samples and combines them with the next chunk.</p> <p>Parameters <pre><code>chunks_iterator : Iterator[bytes]\n    An iterator of byte chunks to aggregate\n</code></pre> Returns <pre><code>Iterator[NDArray[np.int16]]\n    An iterator of 16-bit audio samples\n</code></pre> Example <pre><code>&gt;&gt;&gt; chunks_iterator = [b'\\x00\\x01', b'\\x02\\x03', b'\\x04\\x05']\n&gt;&gt;&gt; for chunk in aggregate_bytes_to_16bit(chunks_iterator):\n&gt;&gt;&gt;     print(chunk)\n</code></pre></p>"},{"location":"utils/#async_aggregate_bytes_to_16bit","title":"<code>async_aggregate_bytes_to_16bit</code>","text":"<p>Aggregate bytes to 16-bit audio samples asynchronously.</p> <p>Parameters <pre><code>chunks_iterator : Iterator[bytes]\n    An iterator of byte chunks to aggregate\n</code></pre> Returns <pre><code>Iterator[NDArray[np.int16]]\n    An iterator of 16-bit audio samples\n</code></pre> Example <pre><code>&gt;&gt;&gt; chunks_iterator = [b'\\x00\\x01', b'\\x02\\x03', b'\\x04\\x05']\n&gt;&gt;&gt; for chunk in async_aggregate_bytes_to_16bit(chunks_iterator):\n&gt;&gt;&gt;     print(chunk)\n</code></pre></p>"},{"location":"utils/#wait_for_item","title":"<code>wait_for_item</code>","text":"<p>Wait for an item from an asyncio.Queue with a timeout.</p> <p>Parameters <pre><code>queue : asyncio.Queue\n    The queue to wait for an item from\ntimeout : float\n    The timeout in seconds\n</code></pre> Returns <pre><code>Any\n    The item from the queue or None if the timeout is reached\n</code></pre></p> <p>Example <pre><code>&gt;&gt;&gt; queue = asyncio.Queue()\n&gt;&gt;&gt; queue.put_nowait(1)\n&gt;&gt;&gt; item = await wait_for_item(queue)\n&gt;&gt;&gt; print(item)\n</code></pre></p>"},{"location":"userguide/api/","title":"Connecting via API","text":"<p>Before continuing, select the <code>modality</code>, <code>mode</code> of your <code>Stream</code> and whether you're using <code>WebRTC</code> or <code>WebSocket</code>s.</p> Connection WebRTC WebSocket Modality Audio Video Audio-Video Mode Send-Receive Receive Send"},{"location":"userguide/api/#sample-code","title":"Sample Code","text":""},{"location":"userguide/api/#message-format","title":"Message Format","text":"<p>Over both WebRTC and WebSocket, the server can send messages of the following format:</p> <pre><code>{\n    \"type\": `send_input` | `fetch_output` | `stopword` | `error` | `warning` | `log`,\n    \"data\": string | object\n}\n</code></pre> <ul> <li><code>send_input</code>: Send any input data for the handler to the server. See <code>Additional Inputs</code> for more details.</li> <li><code>fetch_output</code>: An instance of <code>AdditionalOutputs</code> is sent to the server.</li> <li><code>stopword</code>: The stopword has been detected. See <code>ReplyOnStopWords</code> for more details.</li> <li><code>error</code>: An error occurred. The <code>data</code> will be a string containing the error message.</li> <li><code>warning</code>: A warning occurred. The <code>data</code> will be a string containing the warning message.</li> <li><code>log</code>: A log message. The <code>data</code> will be a string containing the log message.</li> </ul> <p>The <code>ReplyOnPause</code> handler can also send the following <code>log</code> messages.</p> <pre><code>{\n    \"type\": \"log\",\n    \"data\": \"pause_detected\" | \"response_starting\"\n}\n</code></pre> <p>Tip</p> <p>When using WebRTC, the messages will be encoded as strings, so parse as JSON before using.</p>"},{"location":"userguide/api/#additional-inputs","title":"Additional Inputs","text":"<p>When the <code>send_input</code> message is received, update the inputs of your handler however you like by using the <code>set_input</code> method of the <code>Stream</code> object.</p> <p>A common pattern is to use a <code>POST</code> request to send the updated data. The first argument to the <code>set_input</code> method is the <code>webrtc_id</code> of the handler.</p> <pre><code>from pydantic import BaseModel, Field\n\nclass InputData(BaseModel):\n    webrtc_id: str\n    conf_threshold: float = Field(ge=0, le=1)\n\n\n@app.post(\"/input_hook\")\nasync def _(data: InputData):\n    stream.set_input(data.webrtc_id, data.conf_threshold)\n</code></pre> <p>The updated data will be passed to the handler on the next call.</p>"},{"location":"userguide/api/#additional-outputs","title":"Additional Outputs","text":"<p>The <code>fetch_output</code> message is sent to the client whenever an instance of <code>AdditionalOutputs</code> is available. You can access the latest output data by calling the <code>fetch_latest_output</code> method of the <code>Stream</code> object. </p> <p>However, rather than fetching each output manually, a common pattern is to fetch the entire stream of output data by calling the <code>output_stream</code> method.</p> <p>Here is an example: <pre><code>from fastapi.responses import StreamingResponse\n\n@app.get(\"/updates\")\nasync def stream_updates(webrtc_id: str):\n    async def output_stream():\n        async for output in stream.output_stream(webrtc_id):\n            # Output is the AdditionalOutputs instance\n            # Be sure to serialize it however you would like\n            yield f\"data: {output.args[0]}\\n\\n\"\n\n    return StreamingResponse(\n        output_stream(), \n        media_type=\"text/event-stream\"\n    )\n</code></pre></p>"},{"location":"userguide/api/#handling-errors","title":"Handling Errors","text":"<p>When connecting via <code>WebRTC</code>, the server will respond to the <code>/webrtc/offer</code> route with a JSON response. If there are too many connections, the server will respond with a 200 error.</p> <pre><code>{\n    \"status\": \"failed\",\n    \"meta\": {\n        \"error\": \"concurrency_limit_reached\",\n        \"limit\": 10\n    }\n</code></pre> <p>Over <code>WebSocket</code>, the server will send the same message before closing the connection.</p> <p>Tip</p> <p>The server will sends a 200 status code because otherwise the gradio client will not be able to process the json response and display the error.</p>"},{"location":"userguide/audio-video/","title":"Audio-Video Streaming","text":"<p>You can simultaneously stream audio and video using <code>AudioVideoStreamHandler</code> or <code>AsyncAudioVideoStreamHandler</code>. They are identical to the audio <code>StreamHandlers</code> with the addition of <code>video_receive</code> and <code>video_emit</code> methods which take and return a <code>numpy</code> array, respectively.</p> <p>Here is an example of the video handling functions for connecting with the Gemini multimodal API. In this case, we simply reflect the webcam feed back to the user but every second we'll send the latest webcam frame (and an additional image component) to the Gemini server.</p> <p>Please see the \"Gemini Audio Video Chat\" example in the cookbook for the complete code.</p> Async Gemini Video Handling<pre><code>async def video_receive(self, frame: np.ndarray):\n    \"\"\"Send video frames to the server\"\"\"\n    if self.session:\n        # send image every 1 second\n        # otherwise we flood the API\n        if time.time() - self.last_frame_time &gt; 1:\n            self.last_frame_time = time.time()\n            await self.session.send(encode_image(frame))\n            if self.latest_args[2] is not None:\n                await self.session.send(encode_image(self.latest_args[2]))\n    self.video_queue.put_nowait(frame)\n\nasync def video_emit(self) -&gt; VideoEmitType:\n    \"\"\"Return video frames to the client\"\"\"\n    return await self.video_queue.get()\n</code></pre>"},{"location":"userguide/audio/","title":"Audio Streaming","text":""},{"location":"userguide/audio/#reply-on-pause","title":"Reply On Pause","text":"<p>Typically, you want to run a python function whenever a user has stopped speaking. This can be done by wrapping a python generator with the <code>ReplyOnPause</code> class and passing it to the <code>handler</code> argument of the <code>Stream</code> object. The <code>ReplyOnPause</code> class will handle the voice detection and turn taking logic automatically!</p> CodeNotes <pre><code>from fastrtc import ReplyOnPause, Stream\n\ndef response(audio: tuple[int, np.ndarray]): # (1)\n    sample_rate, audio_array = audio\n    # Generate response\n    for audio_chunk in generate_response(sample_rate, audio_array):\n        yield (sample_rate, audio_chunk) # (2)\n\nstream = Stream(\n    handler=ReplyOnPause(response),\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n</code></pre> <ol> <li> <p>The python generator will receive the entire audio up until the user stopped. It will be a tuple of the form (sampling_rate, numpy array of audio). The array will have a shape of (1, num_samples). You can also pass in additional input components.</p> </li> <li> <p>The generator must yield audio chunks as a tuple of (sampling_rate, numpy audio array). Each numpy audio array must have a shape of (1, num_samples).</p> </li> </ol> <ol> <li> <p>The python generator will receive the entire audio up until the user stopped. It will be a tuple of the form (sampling_rate, numpy array of audio). The array will have a shape of (1, num_samples). You can also pass in additional input components.</p> </li> <li> <p>The generator must yield audio chunks as a tuple of (sampling_rate, numpy audio array). Each numpy audio array must have a shape of (1, num_samples).</p> </li> </ol> <p>Asynchronous</p> <p>You can also use an async generator with <code>ReplyOnPause</code>.</p> <p>Parameters</p> <p>You can customize the voice detection parameters by passing in <code>algo_options</code> and <code>model_options</code> to the <code>ReplyOnPause</code> class. <pre><code>from fastrtc import AlgoOptions, SileroVadOptions\n\nstream = Stream(\n    handler=ReplyOnPause(\n        response,\n        algo_options=AlgoOptions(\n            audio_chunk_duration=0.6,\n            started_talking_threshold=0.2,\n            speech_threshold=0.1\n        ),\n        model_options=SileroVadOptions(\n            threshold=0.5,\n            min_speech_duration_ms=250,\n            min_silence_duration_ms=100\n        )\n    )\n)\n</code></pre></p>"},{"location":"userguide/audio/#reply-on-stopwords","title":"Reply On Stopwords","text":"<p>You can configure your AI model to run whenever a set of \"stop words\" are detected, like \"Hey Siri\" or \"computer\", with the <code>ReplyOnStopWords</code> class. </p> <p>The API is similar to <code>ReplyOnPause</code> with the addition of a <code>stop_words</code> parameter.</p> CodeNotes <pre><code>from fastrtc import Stream, ReplyOnStopWords\n\ndef response(audio: tuple[int, np.ndarray]):\n    \"\"\"This function must yield audio frames\"\"\"\n    ...\n    for numpy_array in generated_audio:\n        yield (sampling_rate, numpy_array, \"mono\")\n\nstream = Stream(\n    handler=ReplyOnStopWords(generate,\n                            input_sample_rate=16000,\n                            stop_words=[\"computer\"]), # (1)\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n</code></pre> <ol> <li>The <code>stop_words</code> can be single words or pairs of words. Be sure to include common misspellings of your word for more robust detection, e.g. \"llama\", \"lamma\". In my experience, it's best to use two very distinct words like \"ok computer\" or \"hello iris\". </li> </ol> <ol> <li>The <code>stop_words</code> can be single words or pairs of words. Be sure to include common misspellings of your word for more robust detection, e.g. \"llama\", \"lamma\". In my experience, it's best to use two very distinct words like \"ok computer\" or \"hello iris\". </li> </ol> <p>Extra Dependencies</p> <p>The <code>ReplyOnStopWords</code> class requires the the <code>stopword</code> extra. Run <code>pip install fastrtc[stopword]</code> to install it.</p> <p>English Only</p> <p>The <code>ReplyOnStopWords</code> class is currently only supported for English.</p>"},{"location":"userguide/audio/#stream-handler","title":"Stream Handler","text":"<p><code>ReplyOnPause</code> and <code>ReplyOnStopWords</code> are implementations of a <code>StreamHandler</code>. The <code>StreamHandler</code> is a low-level abstraction that gives you arbitrary control over how the input audio stream and output audio stream are created. The following example echos back the user audio.</p> CodeNotes <pre><code>import gradio as gr\nfrom gradio_webrtc import WebRTC, StreamHandler\nfrom queue import Queue\n\nclass EchoHandler(StreamHandler):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.queue = Queue()\n\n    def receive(self, frame: tuple[int, np.ndarray]) -&gt; None: # (1)\n        self.queue.put(frame)\n\n    def emit(self) -&gt; None: # (2)\n        return self.queue.get()\n\n    def copy(self) -&gt; StreamHandler:\n        return EchoHandler()\n\n    def shutdown(self) -&gt; None: # (3)\n        pass\n\n    def start_up(self) -&gt; None: # (4)\n        pass\n\nstream = Stream(\n    handler=EchoHandler(),\n    modality=\"audio\",\n    mode=\"send-receive\"\n)\n</code></pre> <ol> <li>The <code>StreamHandler</code> class implements three methods: <code>receive</code>, <code>emit</code> and <code>copy</code>. The <code>receive</code> method is called when a new frame is received from the client, and the <code>emit</code> method returns the next frame to send to the client. The <code>copy</code> method is called at the beginning of the stream to ensure each user has a unique stream handler.</li> <li>The <code>emit</code> method SHOULD NOT block. If a frame is not ready to be sent, the method should return <code>None</code>. If you need to wait for a frame, use <code>wait_for_item</code> from the <code>utils</code> module.</li> <li>The <code>shutdown</code> method is called when the stream is closed. It should be used to clean up any resources.</li> <li>The <code>start_up</code> method is called when the stream is first created. It should be used to initialize any resources. See Talk To OpenAI or Talk To Gemini for an example of a <code>StreamHandler</code> that uses the <code>start_up</code> method to connect to an API.    </li> </ol> <ol> <li>The <code>StreamHandler</code> class implements three methods: <code>receive</code>, <code>emit</code> and <code>copy</code>. The <code>receive</code> method is called when a new frame is received from the client, and the <code>emit</code> method returns the next frame to send to the client. The <code>copy</code> method is called at the beginning of the stream to ensure each user has a unique stream handler.</li> <li>The <code>emit</code> method SHOULD NOT block. If a frame is not ready to be sent, the method should return <code>None</code>. If you need to wait for a frame, use <code>wait_for_item</code> from the <code>utils</code> module.</li> <li>The <code>shutdown</code> method is called when the stream is closed. It should be used to clean up any resources.</li> <li>The <code>start_up</code> method is called when the stream is first created. It should be used to initialize any resources. See Talk To OpenAI or Talk To Gemini for an example of a <code>StreamHandler</code> that uses the <code>start_up</code> method to connect to an API.</li> </ol> <p>Tip</p> <p>See this Talk To Gemini for a complete example of a more complex stream handler.</p> <p>Warning</p> <p>The <code>emit</code> method should not block. If you need to wait for a frame, use <code>wait_for_item</code> from the <code>utils</code> module.</p>"},{"location":"userguide/audio/#async-stream-handlers","title":"Async Stream Handlers","text":"<p>It is also possible to create asynchronous stream handlers. This is very convenient for accessing async APIs from major LLM developers, like Google and OpenAI. The main difference is that <code>receive</code>, <code>emit</code>, and <code>start_up</code> are now defined with <code>async def</code>.</p> <p>Here is aa simple example of using <code>AsyncStreamHandler</code>:</p> Code <pre><code>from fastrtc import AsyncStreamHandler, wait_for_item\nimport asyncio\n\nclass AsyncEchoHandler(AsyncStreamHandler):\n    \"\"\"Handler for the Gemini API\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.queue = asyncio.Queue()\n\n    async def receive(self, frame: tuple[int, np.ndarray]) -&gt; None: \n        self.queue.put(frame)\n\n    async def emit(self) -&gt; None: # (2)\n        return await wait_for_item(self.queue)\n\n    def copy(self):\n        return AsyncEchoHandler()\n\n    async def shutdown(self): # (3)\n        pass\n\n    def start_up(self) -&gt; None: # (4)\n        pass\n</code></pre> <p>Tip</p> <p>See Talk To Gemini, Talk To Openai for complete examples of <code>AsyncStreamHandler</code>s.</p>"},{"location":"userguide/audio/#text-to-speech","title":"Text To Speech","text":"<p>You can use an on-device text to speech model if you have the <code>tts</code> extra installed. Import the <code>get_tts_model</code> function and call it with the model name you want to use. At the moment, the only model supported is <code>kokoro</code>.</p> <p>The <code>get_tts_model</code> function returns an object with three methods:</p> <ul> <li><code>tts</code>: Synchronous text to speech.</li> <li><code>stream_tts_sync</code>: Synchronous text to speech streaming.</li> <li><code>stream_tts</code>: Asynchronous text to speech streaming.</li> </ul> <pre><code>from fastrtc import get_tts_model\n\nmodel = get_tts_model(model=\"kokoro\")\n\nfor audio in model.stream_tts_sync(\"Hello, world!\"):\n    yield audio\n\nasync for audio in model.stream_tts(\"Hello, world!\"):\n    yield audio\n\naudio = model.tts(\"Hello, world!\")\n</code></pre> <p>Tip</p> <p>You can customize the audio by passing in an instace of <code>KokoroTTSOptions</code> to the method. See here for a list of available voices. <pre><code>from fastrtc import KokoroTTSOptions, get_tts_model\n\nmodel = get_tts_model(model=\"kokoro\")\n\noptions = KokoroTTSOptions(\n    voice=\"af_heart\",\n    speed=1.0,\n    lang=\"en-us\"\n)\n\naudio = model.tts(\"Hello, world!\", options=options)\n</code></pre></p>"},{"location":"userguide/audio/#speech-to-text","title":"Speech To Text","text":"<p>You can use an on-device speech to text model if you have the <code>stt</code> or <code>stopword</code> extra installed. Import the <code>get_stt_model</code> function and call it with the model name you want to use. At the moment, the only models supported are <code>moonshine/base</code> and <code>moonshine/tiny</code>.</p> <p>The <code>get_stt_model</code> function returns an object with the following method:</p> <ul> <li><code>stt</code>: Synchronous speech to text.</li> </ul> <pre><code>from fastrtc import get_stt_model\n\nmodel = get_stt_model(model=\"moonshine/base\")\n\naudio = (16000, np.random.randint(-32768, 32768, size=(1, 16000)))\ntext = model.stt(audio)\n</code></pre> <p>Example</p> <p>See LLM Voice Chat for an example of using the <code>stt</code> method in a <code>ReplyOnPause</code> handler.</p> <p>English Only</p> <p>The <code>stt</code> model is currently only supported for English.</p>"},{"location":"userguide/audio/#requesting-inputs","title":"Requesting Inputs","text":"<p>In <code>ReplyOnPause</code> and <code>ReplyOnStopWords</code>, any additional input data is automatically passed to your generator. For <code>StreamHandler</code>s, you must manually request the input data from the client.</p> <p>You can do this by calling <code>await self.wait_for_args()</code> (for <code>AsyncStreamHandler</code>s) in either the <code>emit</code> or <code>receive</code> methods. For a <code>StreamHandler</code>, you can call <code>self.wait_for_args_sync()</code>.</p> <p>We can access the value of this component via the <code>latest_args</code> property of the <code>StreamHandler</code>. The <code>latest_args</code> is a list storing each of the values. The 0th index is the dummy string <code>__webrtc_value__</code>.</p>"},{"location":"userguide/audio/#telephone-integration","title":"Telephone Integration","text":"<p>In order for your handler to work over the phone, you must make sure that your handler is not expecting any additional input data besides the audio.</p> <p>If you call <code>await self.wait_for_args()</code> your stream will wait forever for the additional input data.</p> <p>The stream handlers have a <code>phone_mode</code> property that is set to <code>True</code> if the stream is running over the phone. You can use this property to determine if you should wait for additional input data.</p> <pre><code>def emit(self):\n    if self.phone_mode:\n        self.latest_args = [None]\n    else:\n        await self.wait_for_args()\n</code></pre>"},{"location":"userguide/audio/#replyonpause","title":"<code>ReplyOnPause</code>","text":"<p>The generator you pass to <code>ReplyOnPause</code> must have default arguments for all arguments except audio.</p> <p>If you yield <code>AdditionalOutputs</code>, they will be passed in as the input arguments to the generator the next time it is called.</p> <p>Tip</p> <p>See Talk To Claude for an example of a <code>ReplyOnPause</code> handler that is compatible with telephone usage. Notice how the input chatbot history is yielded as an <code>AdditionalOutput</code> on each invocation.</p>"},{"location":"userguide/gradio/","title":"Gradio Component","text":"<p>The automatic gradio UI is a great way to test your stream. However, you may want to customize the UI to your liking or simply build a standalone Gradio application. </p>"},{"location":"userguide/gradio/#the-webrtc-component","title":"The WebRTC Component","text":"<p>To build a standalone Gradio application, you can use the <code>WebRTC</code> component and implement the <code>stream</code> event. Similarly to the <code>Stream</code> object, you must set the <code>mode</code> and <code>modality</code> arguments and pass in a <code>handler</code>.</p> <p>In the <code>stream</code> event, you pass in your handler as well as the input and output components.</p> <pre><code>import gradio as gr\nfrom fastrtc import WebRTC, ReplyOnPause\n\ndef response(audio: tuple[int, np.ndarray]):\n    \"\"\"This function must yield audio frames\"\"\"\n    ...\n    yield audio\n\n\nwith gr.Blocks() as demo:\n    gr.HTML(\n    \"\"\"\n    &lt;h1 style='text-align: center'&gt;\n    Chat (Powered by WebRTC \u26a1\ufe0f)\n    &lt;/h1&gt;\n    \"\"\"\n    )\n    with gr.Column():\n        with gr.Group():\n            audio = WebRTC(\n                mode=\"send-receive\",\n                modality=\"audio\",\n            )\n        audio.stream(fn=ReplyOnPause(response),\n                    inputs=[audio], outputs=[audio],\n                    time_limit=60)\ndemo.launch()\n</code></pre>"},{"location":"userguide/gradio/#additional-outputs","title":"Additional Outputs","text":"<p>In order to modify other components from within the WebRTC stream, you must yield an instance of <code>AdditionalOutputs</code> and add an <code>on_additional_outputs</code> event to the <code>WebRTC</code> component.</p> <p>This is common for displaying a multimodal text/audio conversation in a Chatbot UI.</p> CodeNotes Additional Outputs<pre><code>from fastrtc import AdditionalOutputs, WebRTC\n\ndef transcribe(audio: tuple[int, np.ndarray],\n               transformers_convo: list[dict],\n               gradio_convo: list[dict]):\n    response = model.generate(**inputs, max_length=256)\n    transformers_convo.append({\"role\": \"assistant\", \"content\": response})\n    gradio_convo.append({\"role\": \"assistant\", \"content\": response})\n    yield AdditionalOutputs(transformers_convo, gradio_convo) # (1)\n\n\nwith gr.Blocks() as demo:\n    gr.HTML(\n    \"\"\"\n    &lt;h1 style='text-align: center'&gt;\n    Talk to Qwen2Audio (Powered by WebRTC \u26a1\ufe0f)\n    &lt;/h1&gt;\n    \"\"\"\n    )\n    transformers_convo = gr.State(value=[])\n    with gr.Row():\n        with gr.Column():\n            audio = WebRTC(\n                label=\"Stream\",\n                mode=\"send\", # (2)\n                modality=\"audio\",\n            )\n        with gr.Column():\n            transcript = gr.Chatbot(label=\"transcript\", type=\"messages\")\n\n    audio.stream(ReplyOnPause(transcribe),\n                inputs=[audio, transformers_convo, transcript],\n                outputs=[audio], time_limit=90)\n    audio.on_additional_outputs(lambda s,a: (s,a), # (3)\n                                outputs=[transformers_convo, transcript],\n                                queue=False, show_progress=\"hidden\")\n    demo.launch()\n</code></pre> <ol> <li>Pass your data to <code>AdditionalOutputs</code> and yield it.</li> <li>In this case, no audio is being returned, so we set <code>mode=\"send\"</code>. However, if we set <code>mode=\"send-receive\"</code>, we could also yield generated audio and <code>AdditionalOutputs</code>.</li> <li>The <code>on_additional_outputs</code> event does not take <code>inputs</code>. It's common practice to not run this event on the queue since it is just a quick UI update.</li> </ol> <ol> <li>Pass your data to <code>AdditionalOutputs</code> and yield it.</li> <li>In this case, no audio is being returned, so we set <code>mode=\"send\"</code>. However, if we set <code>mode=\"send-receive\"</code>, we could also yield generated audio and <code>AdditionalOutputs</code>.</li> <li>The <code>on_additional_outputs</code> event does not take <code>inputs</code>. It's common practice to not run this event on the queue since it is just a quick UI update.</li> </ol>"},{"location":"userguide/streams/","title":"Core Concepts","text":"<p>The core of FastRTC is the <code>Stream</code> object. It can be used to stream audio, video, or both.</p> <p>Here's a simple example of creating a video stream that flips the video vertically. We'll use it to explain the core concepts of the <code>Stream</code> object. Click on the plus icons to get a link to the relevant section.</p> <pre><code>from fastrtc import Stream\nimport gradio as gr\nimport numpy as np\n\ndef detection(image, slider):\n    return np.flip(image, axis=0)\n\nstream = Stream(\n    handler=detection, # (1)\n    modality=\"video\", # (2)\n    mode=\"send-receive\", # (3)\n    additional_inputs=[\n        gr.Slider(minimum=0, maximum=1, step=0.01, value=0.3) # (4)\n    ],\n    additional_outputs=None, # (5)\n    additional_outputs_handler=None # (6)\n)\n</code></pre> <ol> <li>See Handlers for more information.</li> <li>See Modalities for more information.</li> <li>See Stream Modes for more information.</li> <li>See Additional Inputs for more information.</li> <li>See Additional Outputs for more information.</li> <li>See Additional Outputs Handler for more information.</li> <li>Mount the <code>Stream</code> on a <code>FastAPI</code> app with <code>stream.mount(app)</code> and you can add custom routes to it. See Custom Routes and Frontend Integration for more information.</li> <li>See Built-in Routes for more information.</li> </ol> <p>Run:</p> UIFastAPI <pre><code>stream.ui.launch()\n</code></pre> <pre><code>app = FastAPI()\nstream.mount(app)\n\n# uvicorn app:app --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"userguide/streams/#stream-modes","title":"Stream Modes","text":"<p>FastRTC supports three streaming modes:</p> <ul> <li><code>send-receive</code>: Bidirectional streaming (default)</li> <li><code>send</code>: Client-to-server only </li> <li><code>receive</code>: Server-to-client only</li> </ul>"},{"location":"userguide/streams/#modalities","title":"Modalities","text":"<p>FastRTC supports three modalities:</p> <ul> <li><code>video</code>: Video streaming</li> <li><code>audio</code>: Audio streaming  </li> <li><code>audio-video</code>: Combined audio and video streaming</li> </ul>"},{"location":"userguide/streams/#handlers","title":"Handlers","text":"<p>The <code>handler</code> argument is the main argument of the <code>Stream</code> object. A handler should be a function or a class that inherits from <code>StreamHandler</code> or <code>AsyncStreamHandler</code> depending on the modality and mode.</p> Modality send-receive send receive video Function that takes a video frame and returns a new video frame Function that takes a video frame and returns a new frame Function that takes a video frame and returns a new frame audio <code>StreamHandler</code> or <code>AsyncStreamHandler</code> subclass <code>StreamHandler</code> or <code>AsyncStreamHandler</code> subclass Generator yielding audio frames audio-video <code>AudioVideoStreamHandler</code> or <code>AsyncAudioVideoStreamHandler</code> subclass Not Supported Yet Not Supported Yet"},{"location":"userguide/streams/#methods","title":"Methods","text":"<p>The <code>Stream</code> has three main methods:</p> <ul> <li><code>.ui.launch()</code>: Launch a built-in UI for easily testing and sharing your stream. Built with Gradio. You can change the UI by setting the <code>ui</code> property of the <code>Stream</code> object. Also see the Gradio guide for building Gradio apss with fastrtc.</li> <li><code>.fastphone()</code>: Get a free temporary phone number to call into your stream. Hugging Face token required.</li> <li><code>.mount(app)</code>: Mount the stream on a FastAPI app. Perfect for integrating with your already existing production system or for building a custom UI.</li> </ul> <p>Warning</p> <p>Websocket docs are only available for audio streams. Telephone docs are only available for audio streams in <code>send-receive</code> mode.</p>"},{"location":"userguide/streams/#additional-inputs","title":"Additional Inputs","text":"<p>You can add additional inputs to your stream using the <code>additional_inputs</code> argument. These inputs will be displayed in the generated Gradio UI and they will be passed to the handler as additional arguments.</p> <p>Tip</p> <p>For audio <code>StreamHandlers</code>, please read the special note on requesting inputs.</p> <p>In the automatic gradio UI, these inputs will be the same python type corresponding to the Gradio component. In our case, we used a <code>gr.Slider</code> as the additional input, so it will be passed as a float. See the Gradio documentation for a complete list of components and their corresponding types.</p>"},{"location":"userguide/streams/#input-hooks","title":"Input Hooks","text":"<p>Outside of the gradio UI, you are free to update the inputs however you like by using the <code>set_input</code> method of the <code>Stream</code> object.</p> <p>A common pattern is to use a <code>POST</code> request to send the updated data.</p> <pre><code>from pydantic import BaseModel, Field\nfrom fastapi import FastAPI\n\nclass InputData(BaseModel):\n    webrtc_id: str\n    conf_threshold: float = Field(ge=0, le=1)\n\napp = FastAPI()\nstream.mount(app)\n\n@app.post(\"/input_hook\")\nasync def _(data: InputData):\n    stream.set_input(data.webrtc_id, data.conf_threshold)\n</code></pre> <p>The updated data will be passed to the handler on the next call.</p>"},{"location":"userguide/streams/#additional-outputs","title":"Additional Outputs","text":"<p>You can return additional output from the handler by returning an instance of <code>AdditionalOutputs</code> from the handler. Let's modify our previous example to also return the number of detections in the frame.</p> <pre><code>from fastrtc import Stream, AdditionalOutputs\nimport gradio as gr\n\ndef detection(image, conf_threshold=0.3):\n    processed_frame, n_objects = process_frame(image, conf_threshold)\n    return processed_frame, AdditionalOutputs(n_objects)\n\nstream = Stream(\n    handler=detection,\n    modality=\"video\",\n    mode=\"send-receive\",\n    additional_inputs=[\n        gr.Slider(minimum=0, maximum=1, step=0.01, value=0.3)\n    ],\n    additional_outputs=[gr.Number()], # (5)\n    additional_outputs_handler=lambda component, n_objects: n_objects\n)\n</code></pre> <p>We added a <code>gr.Number()</code> to the additional outputs and we provided an <code>additional_outputs_handler</code>.</p> <p>The <code>additional_outputs_handler</code> is only needed for the gradio UI. It is a function that takes the current state of the <code>component</code> and the instance of <code>AdditionalOutputs</code> and returns the updated state of the <code>component</code>. In our case, we want to update the <code>gr.Number()</code> with the number of detections.</p> <p>Tip</p> <p>Since the webRTC is very low latency, you probably don't want to return an additional output on each frame. </p>"},{"location":"userguide/streams/#output-hooks","title":"Output Hooks","text":"<p>Outside of the gradio UI, you are free to access the output data however you like by calling the <code>output_stream</code> method of the <code>Stream</code> object.</p> <p>A common pattern is to use a <code>GET</code> request to get a stream of the output data.</p> <pre><code>from fastapi.responses import StreamingResponse\n\n@app.get(\"/updates\")\nasync def stream_updates(webrtc_id: str):\n    async def output_stream():\n        async for output in stream.output_stream(webrtc_id):\n            # Output is the AdditionalOutputs instance\n            # Be sure to serialize it however you would like\n            yield f\"data: {output.args[0]}\\n\\n\"\n\n    return StreamingResponse(\n        output_stream(), \n        media_type=\"text/event-stream\"\n    )\n</code></pre>"},{"location":"userguide/streams/#custom-routes-and-frontend-integration","title":"Custom Routes and Frontend Integration","text":"<p>You can add custom routes for serving your own frontend or handling additional functionality once you have mounted the stream on a FastAPI app.</p> <pre><code>from fastapi.responses import HTMLResponse\nfrom fastapi import FastAPI\nfrom fastrtc import Stream\n\nstream = Stream(...)\n\napp = FastAPI()\nstream.mount(app)\n\n# Serve a custom frontend\n@app.get(\"/\")\nasync def serve_frontend():\n    return HTMLResponse(content=open(\"index.html\").read())\n</code></pre>"},{"location":"userguide/streams/#telephone-integration","title":"Telephone Integration","text":"<p>FastRTC provides built-in telephone support through the <code>fastphone()</code> method:</p> <pre><code># Launch with a temporary phone number\nstream.fastphone(\n    # Optional: If None, will use the default token in your machine or read from the HF_TOKEN environment variable\n    token=\"your_hf_token\",  \n    host=\"127.0.0.1\",\n    port=8000\n)\n</code></pre> <p>This will print out a phone number along with your temporary code you can use to connect to the stream. You are limited to 10 minutes of calls per calendar month.</p> <p>Warning</p> <p>See this section on making sure your stream handler is compatible for telephone usage.</p> <p>Tip</p> <p>If you don't have a HF token, you can get one here.</p>"},{"location":"userguide/streams/#concurrency","title":"Concurrency","text":"<ol> <li>You can limit the number of concurrent connections by setting the <code>concurrency_limit</code> argument.</li> <li>You can limit the amount of time (in seconds) a connection can stay open by setting the <code>time_limit</code> argument.</li> </ol> <pre><code>stream = Stream(\n    handler=handler,\n    concurrency_limit=10,\n    time_limit=3600\n)\n</code></pre>"},{"location":"userguide/video/","title":"Video Streaming","text":""},{"location":"userguide/video/#inputoutput-streaming","title":"Input/Output Streaming","text":"<p>We already saw this example in the Quickstart and the Core Concepts section.</p> CodeNotes Input/Output Streaming<pre><code>from fastrtc import Stream\nimport gradio as gr\n\ndef detection(image, conf_threshold=0.3): # (1)\n    processed_frame = process_frame(image, conf_threshold)\n    return processed_frame # (2)\n\nstream = Stream(\n    handler=detection,\n    modality=\"video\",\n    mode=\"send-receive\", # (3)\n    additional_inputs=[\n        gr.Slider(minimum=0, maximum=1, step=0.01, value=0.3)\n    ],\n)\n</code></pre> <ol> <li>The webcam frame will be represented as a numpy array of shape (height, width, RGB).</li> <li>The function must return a numpy array. It can take arbitrary values from other components.</li> <li>Set the <code>modality=\"video\"</code> and <code>mode=\"send-receive\"</code></li> </ol> <ol> <li>The webcam frame will be represented as a numpy array of shape (height, width, RGB).</li> <li>The function must return a numpy array. It can take arbitrary values from other components.</li> <li>Set the <code>modality=\"video\"</code> and <code>mode=\"send-receive\"</code></li> </ol>"},{"location":"userguide/video/#server-to-client-only","title":"Server-to-Client Only","text":"<p>In this case, we stream from the server to the client so we will write a generator function that yields the next frame from the video (as a numpy array) and set the <code>mode=\"receive\"</code> in the <code>WebRTC</code> component.</p> Code Server-To-Client<pre><code>from fastrtc import Stream\n\ndef generation():\n    url = \"https://download.tsi.telecom-paristech.fr/gpac/dataset/dash/uhd/mux_sources/hevcds_720p30_2M.mp4\"\n    cap = cv2.VideoCapture(url)\n    iterating = True\n    while iterating:\n        iterating, frame = cap.read()\n        yield frame\n\nstream = Stream(\n    handler=generation,\n    modality=\"video\",\n    mode=\"receive\"\n)\n</code></pre>"},{"location":"userguide/webrtc_docs/","title":"FastRTC Docs","text":""},{"location":"userguide/webrtc_docs/#connecting","title":"Connecting","text":"<p>To connect to the server, you need to create a new RTCPeerConnection object and call the <code>setupWebRTC</code> function below. {% if mode in [\"send-receive\", \"receive\"] %} This code snippet assumes there is an html element with an id of <code>{{ modality }}_output_component_id</code> where the output will be displayed. It should be {{ \"a <code>&lt;audio&gt;</code>\" if modality == \"audio\" else \"an <code>&lt;video&gt;</code>\"}} element.</p> <pre><code>// pass any rtc_configuration params here\nconst pc = new RTCPeerConnection();\n{% if mode in [\"send-receive\", \"receive\"] %}\nconst {{modality}}_output_component = document.getElementById(\"{{modality}}_output_component_id\");\n{% endif %}                     \nasync function setupWebRTC(peerConnection) {\n    {%- if mode in [\"send-receive\", \"send\"] -%}      \n    // Get {{modality}} stream from webcam\n    const stream = await navigator.mediaDevices.getUserMedia({\n        {{modality}}: true,\n    })\n    {%- endif -%}\n    {% if mode == \"send-receive\" %}\n\n    //  Send {{ self.modality }} stream to server\n    stream.getTracks().forEach(async (track) =&gt; {\n        const sender = pc.addTrack(track, stream);\n    })\n    {% elif mode == \"send\" %}\n    // Receive {self.modality} stream from server\n    pc.addTransceiver({{modality}}, { direction: \"recvonly\" })\n    {%- endif -%}\n    {% if mode in [\"send-receive\", \"receive\"] %}\n    peerConnection.addEventListener(\"track\", (evt) =&gt; {\n        if ({{modality}}_output_component &amp;&amp; \n            {{modality}}_output_component.srcObject !== evt.streams[0]) {\n            {{modality}}_output_component.srcObject = evt.streams[0];\n        }\n    });\n    {% endif %}\n    // Create data channel (needed!)\n    const dataChannel = peerConnection.createDataChannel(\"text\");\n\n    // Create and send offer\n    const offer = await peerConnection.createOffer();\n    await peerConnection.setLocalDescription(offer);\n\n    // Send offer to server\n    const response = await fetch('/webrtc/offer', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({\n            sdp: offer.sdp,\n            type: offer.type,\n            webrtc_id: Math.random().toString(36).substring(7)\n        })\n    });\n\n    // Handle server response\n    const serverResponse = await response.json();\n    await peerConnection.setRemoteDescription(serverResponse);\n}\n</code></pre> <p>{%if additional_inputs %}</p>"},{"location":"userguide/webrtc_docs/#sending-input-data","title":"Sending Input Data","text":"<p>Your python handler can request additional data from the frontend by calling the <code>fetch_args()</code> method (see here).</p> <p>This will send a <code>send_input</code> message over the WebRTC data channel. Upon receiving this message, you should trigger the <code>set_input</code> hook of your stream. A simple way to do this is with a <code>POST</code> request.</p> <pre><code>@stream.post(\"/input_hook\")\ndef _(data: PydanticBody):\n    stream.set_inputs(data.webrtc_id, data.inputs)\n</code></pre> <p>And then in your client code:</p> <pre><code>const data_channel = peerConnection.createDataChannel(\"text\");\n\ndata_channel.onmessage = (event) =&gt; {\n    event_json = JSON.parse(event.data);\n    if (event_json.type === \"send_input\") {\n        fetch('/input_hook', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n            },\n            body: inputs\n        }\n            )\n        };\n    };\n</code></pre> <p>The <code>set_inputs</code> hook will set the <code>latest_args</code> property of your stream to whatever the second argument is.</p> <p>NOTE: It is completely up to you how you want to call the <code>set_inputs</code> hook. Here we use a <code>POST</code> request but you can use a websocket or any other protocol.</p> <p>{% endif %}</p> <p>{% if additional_outputs %}</p>"},{"location":"userguide/webrtc_docs/#fetching-output-data","title":"Fetching Output Data","text":"<p>Your python handler can send additional data to the front end by returning or yielding <code>AdditionalOutputs(...)</code>. See the docs.</p> <p>Your front end can fetch these outputs by calling the <code>get_outputs</code> hook of the <code>Stream</code>. Here is an example using <code>server-sent-events</code>:</p> <pre><code>@stream.get(\"/outputs\")\ndef _(webrtc_id: str)\n    async def get_outputs():\n        while True:\n            for output in stream.get_output(webrtc_id):\n                # Serialize to a string prior to this step\n                yield f\"data: {output}\\n\\n\"\n            await\n    return StreamingResponse(get_outputs(),  media_type=\"text/event-stream\")\n</code></pre> <p>NOTE: It is completely up to you how you want to call the <code>get_output</code> hook. Here we use a <code>server-sent-events</code> but you can use whatever protocol you want!</p> <p>{% endif %}</p>"},{"location":"userguide/webrtc_docs/#stopping","title":"Stopping","text":"<p>You can stop the stream by calling the following function:</p> <pre><code>function stop(pc) {\n  // close transceivers\n  if (pc.getTransceivers) {\n    pc.getTransceivers().forEach((transceiver) =&gt; {\n      if (transceiver.stop) {\n        transceiver.stop();\n      }\n    });\n  }\n\n  // close local audio / video\n  if (pc.getSenders()) {\n    pc.getSenders().forEach((sender) =&gt; {\n      if (sender.track &amp;&amp; sender.track.stop) sender.track.stop();\n    });\n  }\n\n  // close peer connection\n  setTimeout(() =&gt; {\n    pc.close();\n  }, 500);\n}\n</code></pre>"},{"location":"userguide/websocket_docs/","title":"FastRTC WebSocket Docs","text":"<p>{% if modality != \"audio\" or mode != \"send-receive\" %} WebSocket connections are currently only supported for audio in send-receive mode.</p>"},{"location":"userguide/websocket_docs/#connecting","title":"Connecting","text":"<p>To connect to the server via WebSocket, you'll need to establish a WebSocket connection and handle audio processing. The code below assumes there is an HTML audio element for output playback.</p> <pre><code>// Setup audio context and stream\nconst audioContext = new AudioContext();\nconst stream = await navigator.mediaDevices.getUserMedia({\n    audio: true\n});\n\n// Create WebSocket connection\nconst ws = new WebSocket(`${window.location.protocol === 'https:' ? 'wss:' : 'ws:'}//${window.location.host}/websocket/offer`);\n\nws.onopen = () =&gt; {\n    // Send initial start message with unique ID\n    ws.send(JSON.stringify({\n        event: \"start\",\n        websocket_id: generateId()  // Implement your own ID generator\n    }));\n\n    // Setup audio processing\n    const source = audioContext.createMediaStreamSource(stream);\n    const processor = audioContext.createScriptProcessor(2048, 1, 1);\n    source.connect(processor);\n    processor.connect(audioContext.destination);\n\n    processor.onaudioprocess = (e) =&gt; {\n        const inputData = e.inputBuffer.getChannelData(0);\n        const mulawData = convertToMulaw(inputData, audioContext.sampleRate);\n        const base64Audio = btoa(String.fromCharCode.apply(null, mulawData));\n\n        if (ws.readyState === WebSocket.OPEN) {\n            ws.send(JSON.stringify({\n                event: \"media\",\n                media: {\n                    payload: base64Audio\n                }\n            }));\n        }\n    };\n};\n\n// Handle incoming audio\nconst outputContext = new AudioContext({ sampleRate: 24000 });\nlet audioQueue = [];\nlet isPlaying = false;\n\nws.onmessage = (event) =&gt; {\n    const data = JSON.parse(event.data);\n    if (data.event === \"media\") {\n        // Process received audio\n        const audioData = atob(data.media.payload);\n        const mulawData = new Uint8Array(audioData.length);\n        for (let i = 0; i &lt; audioData.length; i++) {\n            mulawData[i] = audioData.charCodeAt(i);\n        }\n\n        // Convert mu-law to linear PCM\n        const linearData = alawmulaw.mulaw.decode(mulawData);\n        const audioBuffer = outputContext.createBuffer(1, linearData.length, 24000);\n        const channelData = audioBuffer.getChannelData(0);\n\n        for (let i = 0; i &lt; linearData.length; i++) {\n            channelData[i] = linearData[i] / 32768.0;\n        }\n\n        audioQueue.push(audioBuffer);\n        if (!isPlaying) {\n            playNextBuffer();\n        }\n    }\n};\n\nfunction playNextBuffer() {\n    if (audioQueue.length === 0) {\n        isPlaying = false;\n        return;\n    }\n\n    isPlaying = true;\n    const bufferSource = outputContext.createBufferSource();\n    bufferSource.buffer = audioQueue.shift();\n    bufferSource.connect(outputContext.destination);\n    bufferSource.onended = playNextBuffer;\n    bufferSource.start();\n}\n</code></pre> <p>Note: This implementation requires the <code>alawmulaw</code> library for audio encoding/decoding: <pre><code>&lt;script src=\"https://cdn.jsdelivr.net/npm/alawmulaw\"&gt;&lt;/script&gt;\n</code></pre></p>"},{"location":"userguide/websocket_docs/#handling-input-requests","title":"Handling Input Requests","text":"<p>When the server requests additional input data, it will send a <code>send_input</code> message over the WebSocket. You should handle this by sending the data to your input hook:</p> <pre><code>ws.onmessage = (event) =&gt; {\n    const data = JSON.parse(event.data);\n    // Handle send_input messages\n    if (data?.type === \"send_input\") {\n        fetch('/input_hook', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ \n                webrtc_id: websocket_id,  // Use the same ID from connection\n                inputs: your_input_data \n            })\n        });\n    }\n    // ... existing audio handling code ...\n};\n</code></pre>"},{"location":"userguide/websocket_docs/#receiving-additional-outputs","title":"Receiving Additional Outputs","text":"<p>To receive additional outputs from the server, you can use Server-Sent Events (SSE):</p> <pre><code>const eventSource = new EventSource('/outputs?webrtc_id=' + websocket_id);\neventSource.addEventListener(\"output\", (event) =&gt; {\n    const eventJson = JSON.parse(event.data);\n    // Handle the output data here\n    console.log(\"Received output:\", eventJson);\n});\n</code></pre>"},{"location":"userguide/websocket_docs/#stopping","title":"Stopping","text":"<p>To stop the WebSocket connection:</p> <pre><code>function stop(ws) {\n    if (ws) {\n        ws.send(JSON.stringify({\n            event: \"stop\"\n        }));\n        ws.close();\n    }\n}\n</code></pre> <p>{% endif %}</p>"}]}